%*******************************************************************************
%Example chapter file for books, Copyright A K Peters, Ltd.
%*******************************************************************************
\chapter{Exploring Mobile vs. Desktop OpenGL Performance}{Jon McCaffrey}
\label{Exploring-Mobile-vs-Desktop-OpenGL-Performance}

\section{Introduction}

The stunning rise of mobile platforms has created a nascent market for new 3D
applications and games where, excitingly, OpenGL ES is the \textit{lingua
franca} for graphics. However, mobile platforms and GPUs have performance
profiles and characteristics that may be unfamiliar and surprising to desktop
developers.  Developers and applications hoping to make the transition from
desktop to mobile need to be aware of this to create the best experience
possible for given hardware.

\section{Constraint Inspires
Creativity}\label{Jon-McCaffrey:Constraints-Inspire-Creativity}

\subsection{Differences in
Scale}\label{Jon-McCaffrey:Architectural-Differences} Modern mobile devices are
capable and remarkable devices.  However, they face much greater limitations
than desktop systems in terms of cost, die size, power consumption, and heat
dissipation.

Power consumption is a major concern for mobile platforms that is much less
pressing on desktop.  Mobile devices must run off batteries small enough to fit
in the body of the device, and a short battery life is frustrating and
inconvenient to the user.  Mobile hardware is built to use less power than
desktop hardware via lower clock frequencies, narrower busses, smaller chips,
smaller data formats, and by limiting redundant and speculative work.  Display
and network take a great deal of power, but OpenGL applications contribute to
power consumption, especially through compute and through off-chip memory
accesses.

Power consumption is doubly-impactful on mobile devices, since power consumed
by the processor, GPU, and memory is largely dissipated as heat.  Unlike
desktop systems with active air cooling, good air circulation, and large heat
sinks to radiate heat, mobile systems are usually passively cooled and have
contrained bodies with little room for large sinks or radiating fins.  Excess
heat generation is not only potentially damaging to components, it's also
noticeable and bothersome to users of hand-held products.

Die size and cost are also greatly different between mobile and desktop.
High-end desktop GPUs are some of the largest mainstream chips made, with over
3 billion transistors on recent models \cite{Walton10}.  Both the large area
and the effect of area on yield mean increased cost.  The separate chip also
means a separate package and mounting, and the expected cost increase.  In
mobile however, the GPU is usually one component on an integrated
\textit{System-on-a-Chip (SoC)} designed for mobile and embedded applications,
which means it is a fraction of the cost and area of a desktop GPU.

\subsection{Differences in Rendering Architecture}
\label{Jon-McCaffrey:differences-in-rendering-architecture}
\index{tiling} \index{Immediate-Mode Rendering} Mobile and desktop GPUs don't
differ only in scale.  mobile GPUs such as the Imagination Tech SGX543MP2 used
in the Apple iPhone 4S/iPad 2 and the Mali-400 used in the Samsung Galaxy S2
use a \textit{tiled} rendering architecture \cite{Klug11a}.  In
contrast, familiar desktop GPUs from NVIDIA and ATI and mobile GPUs like the
GeForce ULV GPU use in the Samsung Galaxy Tab 10.1 use \textit{Immediate Mode
Rendering (IMR)}.

In IMRs, vertices are transformed once and each primitive is rasterized
essentially in order.  If a fragment passes depth-testing (assuming the
platform has early-z), it will be shaded and will write to the framebuffer.
However, a later fragment may over-write this pixel, nullifying the earlier
work done and writing the framebuffer again.  This issue is known as overdraw.
Even without overdraw, the depth buffer still must be read in order to reject
fragments.

Tilers instead divide the framebuffer into tiles of pixels.  All draw commands
are buffered, or deferred.  At the end of the frame, for each tile, all
geometry for the scene is re-rendered and rasterized into a framebuffer cache
with that tile scissored out.  Once all pixels have been resolved, the entire
tile is written out to memory.  This saves redundant framebuffer writes and
allows for fast depth-buffer access, since depth-testing and depth writes can
be performed with the local framebuffer cache.

\index{Tile-Based Deferred Rendering} There is an additional group of tilers
which use \textit{Tile-Based Deferred Rendering}, including the SGX family.
The idea is to rasterize all primitives in a tile before performing any
fragment shadering.  This allows \textit{Hidden Surface Removal (HSR)} and
depth-testing to be performed in a fast framebuffer cache before any fragment
shading work is done.  Assuming opaque geometry, each pixel is then shaded and
writted to the framebuffer exactly once.

These architectures have different strengths and weaknesses we will consider,
but it's important to note that the mobile GPU landscape is not homogenous.
Optimizations may affect the different architectures very differently, so it is
important to test on multiple devices for cross-plattform releases.
\subsection{Differences in Memory
Architecture}\label{Jon-McCaffrey:differences-in-memory-architecture}

On desktop systems, high-end GPUs are discrete devices which communicate with
the rest of the system via a peripheral bus like \textit{PCIe}.  For good
performance, this means that GPUs must include their own dedicated memory.
While this increases cost, it is an optimization opportunity since this memory
can be optimized for graphics workloads.

For example, the NVIDIA \textit{Fermi} architecture uses GDDR5 memory that is
heavily partitioned \cite{Walton10} to allow for very wide memory interface.
There is also no competition for this bandwidth, since except for uploads from
the rest of the system and scan-out for output devices, the GPU is the only
user of this memory.

\index{System-on-a-Chip} \index{Unified Memory Architecture}

In mobile devices, on the other hand, the GPU is usually integrated into the
same SoC as the CPU and other components, and to save cost, power, size, and
complexity, shares the same RAM and memory interface.  This is known as an
\textit{Unified Memory Architecture (UMA)}.  A common memory type might be the
low-power LPDDR2, which has a 32-bit wide interface
\cite{Klug11b}.  Not only is this memory general-purpose, the GPU
now shares bandwidth with other parts of the system like the CPU, network,
camera, multimedia, and display.   

\index{bandwidth}

There are some performance advantages to a unified memory architecture, besides
the savings in cost and complexity.  With discrete GPU's the peripheral bus
could become a bottleneck for transfers, especially with non-PCIe buses with
asymmetric speeds \cite{Elhasson05}.  With a UMA, OpenGL client and server data
are in fact stored in the same RAM.  Even if it is not possible to directly
access server data with \texttt{glMapBufferOES}, there are fewer performance
cliffs lurking in transfers between OpenGL client and server data.

\index{Pixel Buffer Object}

One limitation is that OpenGL ES does not yet have an extension for
\textit{Pixel Buffer Objects (PBO)}, meaning that pixel data must be
transferred synchronously.  This makes the comparatively cheap bandwidth
between client and server data less useful, and also makes streaming texture
assets more difficult.

\section{Reducing Memory
Bandwidth}\label{Jon-McCaffrey:Reducing-Memory-Bandwidth}

\index{bandwidth}

Memory bandwidth pressure is one of the major performance pressures on mobile
devices, especially on games and other applications which must also perform
heavy amounts of client-side work during the frame.

Besides limiting performance, memory accesses external to the GPU consume a
great deal of power, sometimes more than the compute itself \cite{Antochi04}

\begin{table}[htb]\centering \begin{tabular}{|c|c|c|c|} 
\hline \small{Device} & \small{CPU Write Bandwidth} & \small{GPU Write Bandwidth}   \\ \hline 
\hline \small{Motorola Droid X} & \small{1.4GBps} & \small{6.8GBps} \\ 
\hline \small{LG Thunderbolt} & \small{0.866GBps} & \small{0.518GBps} \\ 
\hline \small{Dell Inspiron 520} & \small{4.8GBps} & \small{\%3.8GBps} \\ 
\hline \small{Desktop System*} & \small{14.2GBps} & \small{} & \small{25.7GBps}\\
\end{tabular} 
\caption{Write bandwidth for CPU and GPU on different devices.  CPU write bandwidth estimated by memset.  GPU bandwidth estimated by glClear+glFinish.  Desktop system has an Intel Core 2 Quad and NVIDIA 8800 GTS.}
\label{JonMcCaffrey:bandwidth} \end{table}

\subsection{Relative Display Sizes}\label{Jon-McCaffrey:relative-display-sizes}

\index{resolution} \index{display}

Despite the tight power and cost constraints for mobile devices, the display
sizes of modern mobile devices are a considerable fraction of the size of
desktop displays.  

\begin{table}[htb]\centering \begin{tabular}{|c|c|c|c|} 
\hline \small{Device} & \small{Resolution} & \small{\% of 1280x1024 (20 in)} & \small{\% of 1920x1080 (24 in)}  \\ \hline 
\small{Motorola XOOM} & \small{1280x800} & \small{\%78.13} & \small{\%49.38}\\ 
\hline \small{Apple iPad 2} & \small{1024x768} & \small{\%58.63} & \small{\%37.06}\\ 
\hline \small{Apple iPhone 4S} & \small{960x640} & \small{\%46.89} & \small{\%29.63}\\
\hline \small{Samsung Galaxy S2} & \small{800x480} & \small{\%30.00} & \small{\%18.52}\\ \hline
\end{tabular} 
\caption{Resolution comparison of desktop and mobile displays} 
\label{JonMcCaffrey:resolutions} \end{table}

\index{fragment shading}

With the limited fragment compute throughput and memory bandwidth of mobile
devices, these comparatively large display sizes mean that fragment shading and
full-screen (or large-quad) operations can easily become a bottleneck.  Memory
bandwidth is also a major power drain, making limiting bandwidth doubly
important.

There is also a large spread of resolution sizes within mobile devices, so
testing on multiple devices is important, for performance testing as well as
useabilty concerns.

\subsection{Framebuffer Bandwidth}\label{Jon-McCaffrey-Framebuffer-Bandwidth}

Basic rendering can consume significant amounts of memory bandwidth.  Memory
bandwidth besides being a performance limit, memory bandwidth is also a
significant power drain, so it is particularly important to limit it.  Assume
the common configuration of 16-bit color with 16-bit depth
\cite{Google11}, with a reasonable 1024x768 display.  Accessing every
pixel in the framebuffer 60 times a second takes 94MB/s of bandwidth.  So
simply to write all the pixels in a scene every frame, at 60 frames a second,
with 0\% overdraw, takes 94MB/s of bandwidth.

\index{bandwidth} \index{depth}

However, assuming an IMR architecture, to be able to render a scene, we also
also usually perform a depth-buffer read for each rendered pixel.  Both the
depth-buffer and the color buffer are also usually cleared each frame.  And
when applications write to the color buffer while rendering the scene, they
also generally write the fragment depth to the depth-buffer.

The memory bandwidth consumption of the final framebuffer doesn't end when the
application is done writing it either.  After \texttt{eglSwapBuffers}, it may
need to be composited by the platform-specific windowing system, and then
scanned out to the display.  Unlike on desktop systems which often have
dedicated graphics or framebuffer memory, this will also consume system memory
bandwidth.  This will consume an additional 96MB/s of bandwidth just for
scanout, or at least an additional 288MB/s with composition(read, write, and
scan-out).

\index{composition}

Thus basic clear-fill-and-post operation consumes 564-752mb/s of bandwidth, so
even basic operations consume a significant amount of memory bandwidth;
anything interesting our application is doing only costs more bandwidth.  If a
32-bit framebuffer is used, this number will be even greater.  To put this in
context, the ipad2 has been benchmarked at 2.3GB/s for stdlib writes
\cite{Shimpi11}  \ref{JonMcCaffrey:bandwidth} for more comparisons.

\index{color depth}

Applications using a 32-bit framebuffer that may be bandwidth-bound should
experiment with a lower-precision format.  Since the output framebuffer is not
often used in subsequent calculations, the loss of numerical precision is not
propagated and magnified.  One valid concern is banding or quantization of
smooth gradients \cite{Guy10}.  However, this may be more of an issue
in graphics, photography and compositing applications rather than games and
3d-applications, just because of the nature of the produced content.

\subsection{Texture Bandwidth}\label{JonMcCaffrey-Texture-Bandwidth} Since
texture accesses are often performed at least once per-pixel, these can be
another large source of bandwidth consumption.

\index{bandwidth}

One simple way to reduce bandwidth is to lower the texture resolution.  Fewer
texels, besides a smaller memory footprint, means better texture cache
utilization and more efficient filtering.  The framebuffer resolution usually
can't be lowered, since native resolution is expected.  Texture sizes are more
flexible.  If assets have been ported from desktop, there may be room for
optimization here.

\index{texture compression}

For static textures (as opposed to textures resulting from off-screen render
targets) texture compression is another great way to save bandwidth,
loading-time, memory footprint, and disk space.  Even though work must be done
to decompress the texture data when it is used, the smaller size of compressed
textures makes them friendlier to texture caching and memory bandwidth,
increasing run-time performance. 

One complication is that there are multiple incompatible formats for texture
compression supported via OpenGL ES2 extensions.  Example formats would be
S3TC, available on NVIDIA Tegra, and PVRTC, available on ImaginationTech SGX
\cite{Motorola11}.

To support texture compression on multiple devices, an application must either
package multiple versions of its assets and dynamically choose the correct
ones, or perform the compression at run-time/install-time.  Performing the
compression at run or install-time must be done carefully to not slow down the
application, and gives up many of the benefits of improved loading-time and
disk-space, as well as the internet bandwidth required for
installation/download of the application on mobile devices.  S3TC has
compression ratios between 4:1 and 8:1 \cite{Domine00}.

\section{Reducing Fragment Workload}
\label{Jon-McCaffrey-Reducing-Fragment-Workload}

\index{fragment}

Due to the limited compute and bandwidth available on mobile devices with
respect to the large number of pixels and the complexity of modern rendering,
fragment shading is often a bottleneck for mobile GPUs.  However, fragment
shading can be improved in other ways than just simplifying shading.

\subsection{Overdraw and Blending}\label{Jon-McCaffrey-Overdraw-And-Blending}
\index{overdraw}\index{blending}\index{IMR}\index{TBDR} Overdraw is when pixels
that have previously been shaded are overwritten by later fragments in a scene.
On IMRs, overdraw wastes fragment shading work, since the previous computed
pixel value is over-written and lost.  The additional framebuffer writes for
over-written pixels also waste bandwidth.

On IMR GPUs, this extra bandwidth consumption and fragment work can be limited
by sorting and rendering geometry from front to back.  This is especially
practical for static geometry which can be processed into a spatial data
structure during export.  An additional heuristic for games is to render the
player character first and the sky-box last.  \cite{Pranckevicius11a}.  For
batches where front-to-back object sorting is not practical, for example with
complicated, interlocking geometry or heavy use of alpha-testing, a depth
pre-pass can be used to eliminate redundant pixel calculations, at the cost of
repeated vertex shading work, primitive assembly, and depth-buffer access.

The idea of a depth-pre-pass is to bind a trivial fragment shader and render
the scene with color writes disabled.  Depth testing proceeds as normal and
fragment ordering is resolved.  The normal fragment shader is then bound and
the scene is re-rendered.  In this manner, only the final fragments that affect
the scene color are rendered.  Clearly, this only works for opaque objects.

Even without overdraw, heavy amounts of overlapping geometry can still be
expensive because of the depth-buffer reads needed to reject pixels.  Primitive
assembly, rasterization, and the pixel reject rate can also become limiting for
large areas like sky-boxes \cite{Pranckevicius11a}.

One type of effect that can be particular expensive in terms of fragment
shading and read and write bandwidth is particle effects rendered via multiple
overlapping quads with blending.  These often overlap and are blended multiple
layers deep.  Each layer of overlap requires a read and write of the existing
framebuffer value, and an additional fragment computation and blending
operation.  One simple fix is tuning the size, count and style of particles.

\subsection {Full-Screen Effects}\label{Jon-McCaffrey-Full-Screen-Effects}
Full-screen post-processing effects are a major tool for visual effects in
modern games and graphics applications, and have been an area of innovation in
recent years.   Common applications of full-screen post-processing in games are
motion-blur, depth-of-field, screen-space ambient occlusion, light bloom, color
filtering, and tone-mapping.  Other applications such as photo-editing tools
may use full-screen or large-area effects for composition, blending, distortion
and filtering.

Full-screen post-processing is a powerful tool to create effects but it is an
easy way to consume large amounts of bandwidth and fragment processing.  Such
effects should be carefully weighed for their worth, and are prime candidates
for optimization.  

A full-screen pass implies at least a read and write of the framebuffer at full
resolution, which at 16-bit color and a 1024x768 resolution means 188MB/s
bandwidth.  One way to optimize these effects is to remove the extra
full-screen pass.  Some post-processing effects such as color-filtering or
tone-mapping that don't require knowledge of neighboring pixels or feedback
from rendering may be merged into the fragment shaders for the objects
themselves.  This may require the use of \textit{uber-shaders} or shader
generation, to allow for natural editing of object fragment shaders while
appending post-processing effects.

If the additional pass cannot be eliminated, then all full-screen post effects
can be merged into a single pass.  This saves redundant round-trips to
framebuffer memory.

\begin{table}[htb]\centering \begin{tabular}{|c||c|c|c|c|} 
\hline \small{Device} & \small{color\_clear} & \small{vertex\_lighting} & \small{one\_tap\_post} & \small{five\_tap\_post}  \\ \hline 
\hline \small{Motorola Droid X} & \small{3.67GPps} & \small{234MPps}& \small{5.36MPps\footnotemark[1]} & \small{5.7MPps\footnotemark[1]} \\ 
\hline \small{LG Thunderbolt} & \small{305MPps} & \small{48.7MPps}& \small{30MPps} & \small{20.36MPps} \\ 
\hline \small{Dell Inspiron 520} & \small{1.92GPps} & \small{231MPps}& \small{139MPps} & \small{120MPps} \\ 
\hline \small{Desktop System*} & \small{13.8GPps} & \small{2.95GPps}& \small{1.73GPps} & \small{1.29GPps} \\ 
\end{tabular} 
\caption{Performance for different shading and passes.  All tests used 1024x1024 16-bit offscreen depth and color buffers as the main framebuffer, with a 32-bit RGBA intermediate color buffer and 16-bit depth buffer where applicable.  vertex\_lighting renders a synthetic scene with vertex lighting, a per-pixel texture lookup, and 39200 triangles with 0\% overdraw.  Five\_tap\_post and one\_tap\_post draw the same scene with five- and one- sample full-screen post-processing passes, respectively.  All units are pixels per second.  Desktop system has an Intel Core 2 Quad and NVIDIA 8800 GTS.}
\footnotetext[1] {The DroidX post-processing scores are low enough that I suspect an implicit render target-to-texture format conversion must be taking place}
\label{JonMcCaffrey:pass_performance} \end{table}

\index{deferred shading}

One limitation of OpenGL ES 2.0 is the poor support for \textit{Multiple Render
Targets (MRT)}, which allow multiple output buffers from a fragment shader.
This makes deferred shading impractical, since running a full pass of the
scene for each \textit{geometry buffer} is too expensive, but only 1 geometry
buffer can be rendered at a time.  Even if MRTs were available however, the
additional bandwidth cost of reading and writing multiple full-screen buffers
would probably rule out deferred shading as a possibility.

\subsection{Off-screen Passes}
\label{Jon-McCaffrey-Off-Screen-Pass}
\index{bloom}
\index{texture filtering}

Similar to full-screen effects are effects requiring off-screen render targets
like environmental reflections, depth-map shadows, and light bloom.

One way to optimize full-screen effects that require a blurred image is to take
advantage of texture filtering hardware.  Rather than rendering a large
offscreen image, then taking multiple samples in a fragment shader to blur, the
scene can be rendered into a low-resolution offscreen target and blurred via
texture filtering.

The main fragment shader for the scene can then bind that target as a texture
and read from it with an appropriate texture-filtering mode such as GL\_LINEAR.
The smaller size of the offscreen target makes this strategy particularly
cache-friendly.  This may work well for light bloom and environmental
reflection, for example.  Depending on the effect, an additional Gaussian
blurring pass on the off-screen target may be needed, but these can also be
accelerated with texture filtering \cite{Rideout}.

Even when the blurring due to texture filtering is not beneficial, reducing
off-screen target resolution may be an easy way to reduce the fragment workload
and memory bandwidth without a serious visual impact.

Whenever moving additional computations into the fragment shader of objects in
the scene, it is important on non-tiled architecture to minimize over-draw to
avoid wasted work.  One advantage of full-screen post-processing in a separate
pass is that each pixel is computed exactly once.

\subsection{Shaving Fragment Work}
\label{Jon-McCaffrey-Shaving-Fragment-Work}

One optimization with a significant amount of leverage is optimizing fragment
shaders.  Shaders tend to be fairly small and simple, but the sheer number of
pixels and amount of computation makes non-trivial fragment shading a major
bottleneck on both TBDR and IMR GPUs.  Optimizations here will probably have
some effect on visual quality, but it may well be worth the gain in
performance.

For static geometry, baking most of the illumination into light-maps will save
computation at run-time, and allow the use of more advanced lighting techniques
than would otherwise be affordable \cite{Miller99} \cite{Unity11}.
This does require a well-developed asset pipeline however.

Another classic trick to avoid floating-point work and special functions in
fragment shaders is to approximate a complicated function with a look-up
texture \cite{Pranckevicius11b}.  This allows the use of much more elaborate
BDRF's.  This also allows for effects that would be difficult to achieve purely
procedurally \cite{Mitchell07}.  1-D look-up textures may be particular
cache-friendly, and with a smooth input parameter should have good locality of
reference.  

However, fragment shaders with multiple texture fetches may already be bound by
texture fetch.  Large amounts of state per fragment may also limit the number
of in-flight fragments, which affects the ability of the GPU to hide texture
fetch latency.

\subsection{Vertex vs. Fragment Work}
\label{Jon-McCaffrey-Vertex-vs-Fragment-Work}

For lighting and shading the primary objects in our scene, traditional IMR
wisdom states that moving computations like lighting, specularity, and
normalization from per-fragment to per-vertex and then interpolating can save
performance at the cost of image quality, and this is still very true for IMR.

However, for TBDRs, this performance wisdom is more dubious.  This is because
mobile devices do have smaller screens with fewer pixels to be shaded that
desktop, and because TBDRs must perform all vertex computations for each tile
\cite{Apple11}.  TBDRs are more likely to be vertex-bound, and Unity
recommends 40k or less vertices on recent iOS devices, which use Imagination
Tech SGX GPUs \cite{unity_graphics_perf}.

This means that heavy vertex shaders, even if they save fragment work, may be a
performance drag on TBDR.  This is particularly true since TBDRs since they
perform little-to-no redundant fragment work.  When working with IMRs, lifting
computation from the fragment shader to the vertex shader is likely a
performance win, and becoming vertex-bound is somewhat less of a concern.

Another consideration to relationship between vertex and fragment shaders is
that adding too many additional varyings can be a drag on performance, since
they must all be interpolated, and a large amount of per-fragment memory may
limit the number of fragments that can be in-flight at once.  A large number of
varyings may also thrash the post-transform cache, which stores the results of
vertex shading, making vertex processing more expensive.  So thinning the
interface between vertex and fragment shading can be valuable.

Vertex processing is more of a bandwidth drain on TBDRs, since the attributes
probably must be pulled again for each tile (unless they hit in a pre- or post-
transform cache)  To lower this bandwidth, a lower-precision buffer format may
be used such as \textit{OES\_vertex\_half\_float}.  Interleaved vertex data,
which interleaves the attributes for each vertex in the same buffer, is also
much more efficient to fetch, since an entire vertex can be fetched in one
linear read \cite{Apple11}.  If there is a pre-transform vertex attribute
cache, which stores fetched vertex attributes (with some locality), this will
also use it more efficiently.  

\section{Conclusion}\label{Jon-McCaffrey-Conclusion}





TODO Probably should get number for effect of silicon process improvement on power.

TODO Conclusion not complete

\bibliographystyle{akpbib} \bibliography{YourBib}











